{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "c7961fdd-84f1-4039-a778-1882435225a0",
      "cell_type": "code",
      "source": "import requests\nfrom bs4 import BeautifulSoup\nimport csv\n\n\ndef scrape_uci_datasets():\n    base_url = \"https://archive.ics.uci.edu/datasets\"\n\n\n    headers = [\n        \"Dataset Name\", \"Donated Date\", \"Description\",\n        \"Dataset Characteristics\", \"Subject Area\", \"Associated Tasks\",\n        \"Feature Type\", \"Instances\", \"Features\"\n    ]\n\n\n    # List to store the scraped data\n    data = []\n\n\n    def scrape_dataset_details(dataset_url):\n        response = requests.get(dataset_url)\n        soup = BeautifulSoup(response.text, 'html.parser')\n\n\n        dataset_name = soup.find(\n            'h1', class_='text-3xl font-semibold text-primary-content')\n        dataset_name = dataset_name.text.strip() if dataset_name else \"N/A\"\n\n\n        donated_date = soup.find('h2', class_='text-sm text-primary-content')\n        donated_date = donated_date.text.strip().replace(\n            'Donated on ', '') if donated_date else \"N/A\"\n\n\n        description = soup.find('p', class_='svelte-17wf9gp')\n        description = description.text.strip() if description else \"N/A\"\n\n\n        details = soup.find_all('div', class_='col-span-4')\n\n\n        dataset_characteristics = details[0].find('p').text.strip() if len(\n            details) > 0 else \"N/A\"\n        subject_area = details[1].find('p').text.strip() if len(\n            details) > 1 else \"N/A\"\n        associated_tasks = details[2].find('p').text.strip() if len(\n            details) > 2 else \"N/A\"\n        feature_type = details[3].find('p').text.strip() if len(\n            details) > 3 else \"N/A\"\n        instances = details[4].find('p').text.strip() if len(\n            details) > 4 else \"N/A\"\n        features = details[5].find('p').text.strip() if len(\n            details) > 5 else \"N/A\"\n\n\n        return [\n            dataset_name, donated_date, description, dataset_characteristics,\n            subject_area, associated_tasks, feature_type, instances, features\n        ]\n\n\n    def scrape_datasets(page_url):\n        response = requests.get(page_url)\n        soup = BeautifulSoup(response.text, 'html.parser')\n\n\n        dataset_list = soup.find_all(\n            'a', class_='link-hover link text-xl font-semibold')\n\n\n        if not dataset_list:\n            print(\"No dataset links found\")\n            return\n\n\n        for dataset in dataset_list:\n            dataset_link = \"https://archive.ics.uci.edu\" + dataset['href']\n            print(f\"Scraping details for {dataset.text.strip()}...\")\n            dataset_details = scrape_dataset_details(dataset_link)\n            data.append(dataset_details)\n\n\n    # Loop through the pages using the pagination parameters\n    skip = 0\n    take = 10\n    while True:\n        page_url = f\"https://archive.ics.uci.edu/datasets?skip={skip}&take={take}&sort=desc&orderBy=NumHits&search=\"\n        print(f\"Scraping page: {page_url}\")\n        initial_data_count = len(data)\n        scrape_datasets(page_url)\n        if len(\n                data\n        ) == initial_data_count: \n            break\n        skip += take\n\n\n    with open('uci_datasets.csv', 'w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        writer.writerow(headers)\n        writer.writerows(data)\n\n\n    print(\"Scraping complete. Data saved to 'uci_datasets.csv'.\")\n\n\nscrape_uci_datasets()",
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c7af26fa-8ae5-4eb8-a0ac-af18a40b7431",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}